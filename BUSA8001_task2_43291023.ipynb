{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BUSA8001 Applied Predictive Analytics - Programming Task 2\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment Points**: 100  \n",
    "**Assignment Weight**: 10%  \n",
    "**Due Date**: Fri, 25 March 2022 @ 11.59pm  \n",
    "**Submission**: Submit your file using the submission link on iLearn\n",
    "\n",
    "\n",
    "Put **all your work** into a file titled `task2_MQ_ID.ipynb` where MQ_ID is your Macquarie University student ID number (e.g. if MQ_ID == 12345678 then you need to submit task2_12345678.ipynb). \n",
    "\n",
    "- Failure to submit a correctly named file will result in a loss of 30 points.\n",
    "- Failure to supply solutions in the cells provided below each question will result in a loss of 30 points.\n",
    "- Follow all instructions closely and not print your variables to screen unless explicitly asked to do so. Failure to do so will result in additional point deductions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 1 - (30 points)\n",
    "\n",
    "**Perform the following tasks in python, writing your code in the cells provided underneath each question.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Import the credit card data from https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls directly into a pandas DataFrame named `df` making sure you skip the top row when reading the dataset. Delete the 'ID\" column after importing the data. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # import pandas library \n",
    "column_names = ['ID','LIMIT_BAL','SEX','EDUCATION','MARRIAGE','AGE','PAY_0','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6','BILL_AMT1','BILL_AMT2','BILL_AMT3','BILL_AMT4','BILL_AMT5','BILL_AMT6','PAY_AMT1','PAY_AMT2','PAY_AMT3','PAY_AMT4','PAY_AMT5','PAY_AMT6','default payment next month']\n",
    "\n",
    "df = pd.read_excel('https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls', skiprows=1, names=column_names)\n",
    "\n",
    "df = df.drop(['ID'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Q2. Rename the column 'PAY_0' to 'PAY_1' and the column 'default payment next month' to 'payment_default' (5 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\"PAY_0\":\"PAY_1\", \"default payment next month\":\"payment_default\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Q3. Create a one-dimensional NumPy array named `y` by exporting the first 12,500 observations of 'payment_default' column from df (hint: see `ravel` NumPy method). Similarly, create a two-dimensional NumPy array named `X` by exporting the first 12,500 observatations of 'PAY_1', 'PAY_2', 'AGE', 'SEX', 'MARRIAGE', 'EDUCATION' and 'BILL_AMT1' columns. (10 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# First 12,500 observations of 'payment_default' into 1D numpy array (y)\n",
    "y = df.loc[0:12499, ['payment_default']].values\n",
    "y = np.ravel(y)\n",
    "\n",
    "\n",
    "# First 12,500 observations into 2D numpy array X\n",
    "X = df.loc[0:12499, ['PAY_1', 'PAY_2', 'AGE', 'SEX', 'MARRIAGE', 'EDUCATION', 'BILL_AMT1']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Q4. Use an appropriate `scikit-learn` library we learned in class to create the following NumPy arrays: `y_train`, `y_test`, `X_train` and `X_test` by splitting the data into 68% train and 32% test datasets. Set `random_state` to 3 and stratify subsamples so that train and test datasets have roughly equal proportions of the target's class labels. (5 points) \n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.32, random_state = 3, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Q5. Use an appropriate `scikit-learn` library we learned in class to standardize features from train and test datasets to mean zero and variance one, as discussed in class. (5 points)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature standardization for train and test datasets\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "\n",
    "sc.fit(X_train) # only scale features not target\n",
    "\n",
    "X_train_scaled = sc.transform(X_train)\n",
    "\n",
    "X_test_scaled = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Problem 2 - (30 Points)\n",
    "\n",
    "Q6. Using approapriate `scikit-learn` libararies we learned in class to fit the following classifiers to the training dataset constructed in Problem 1. \n",
    "\n",
    "- Logistic Regression - name your instance `lr` set `random_state=11`\n",
    "- Support Vector Machine with Linear Kernel - name your instance `svm_linear` set `C=6.0` and `random_state=11`\n",
    "- Support Vector Machine with RBF Kernel - name your instance `svm_rbf` set `gamma = 21`, `C=5.6`, `random_state=11`\n",
    "- Decision Tree - name your instance `tree` set `criterion='entropy'`, `max_depth = 4`, `random_state=11`\n",
    "- Random Forest - name your instance  `forest` set `criterion='entropy'`, `n_estimators=21`, `random_state=11`\n",
    "- KNN - name your instance `knn` set `n_neighbors=6`, `p=3`, `metric='minkowski'`\n",
    "    \n",
    "When initializing instances of the above classifiers only set parameters provided above and leave all other parameters equal to their `scikit-learn` default values.  (30 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=11)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression with random_state = 11\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(random_state=11)\n",
    "\n",
    "lr.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=6.0, kernel='linear', random_state=11)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM_linear with C = 6.0, random_state = 11\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_linear = SVC(kernel='linear', C=6.0, random_state=11)\n",
    "\n",
    "svm_linear.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=5.6, gamma=21, random_state=11)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM_RBF with C = 6.0, random_state = 11\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_rbf = SVC(kernel='rbf', gamma = 21, C=5.6, random_state=11) #rbf is the default for SVM\n",
    "\n",
    "svm_rbf.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', max_depth=4, random_state=11)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decision Tree with criterion = 'entropy', max_depth = 4, random_state = 11\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(criterion='entropy', max_depth = 4, random_state=11)\n",
    "\n",
    "tree.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', n_estimators=21, random_state=11)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random forest with criterion = 'entropy', n_estimators = 21, random_state = 11\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier(criterion='entropy', n_estimators=21, random_state=11)\n",
    "\n",
    "forest.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=6, p=3)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KNN with n_neighbors = 6, p = 3, metric = 'minkowski'\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=6, p=3, metric='minkowski')\n",
    "\n",
    "knn.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 3 - (40 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. Using a method built into each of the above classifiers, compute prediction accuracy on training data for each classifier and store it into variables named according to the following pattern: `classifier_name_accuracy_train`, for instance you should have `lr_accuracy_train`. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train accuracy score for each classifier\n",
    "\n",
    "lr_accuracy_train = lr.score(X_train_scaled, y_train)\n",
    "\n",
    "SVM_linear_accuracy_train = svm_linear.score(X_train_scaled, y_train)\n",
    "\n",
    "SVM_rbf_accuracy_train = svm_rbf.score(X_train_scaled, y_train)\n",
    "\n",
    "tree_accuracy_train = tree.score(X_train_scaled, y_train)\n",
    "\n",
    "forest_accuracy_train = forest.score(X_train_scaled, y_train)\n",
    "\n",
    "knn_accuracy_train = knn.score(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Q8. Using a method built into each of the above classifiers, compute prediction accuracy on test data for each classifier and store it into variables named according to the following pattern: `classifier_name_accuracy_test`, for instance you should have `lr_accuracy_test`. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test accuracy score for each classifier\n",
    "\n",
    "lr_accuracy_test = lr.score(X_test_scaled, y_test)\n",
    "\n",
    "SVM_linear_accuracy_test = svm_linear.score(X_test_scaled, y_test)\n",
    "\n",
    "SVM_rbf_accuracy_test = svm_rbf.score(X_test_scaled, y_test)\n",
    "\n",
    "tree_accuracy_test = tree.score(X_test_scaled, y_test)\n",
    "\n",
    "forest_accuracy_test = forest.score(X_test_scaled, y_test)\n",
    "\n",
    "knn_accuracy_test = knn.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Q9. Explain which methods rank in the first two places according to their ability to accurately classify train data, and which two methods perform worst on train dataset? (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---------- ANSWER -----------------\n",
    "\n",
    "\n",
    "\n",
    "The methods that rank in the first two places in terms of accurately classifying train data are:\n",
    "- Random Forest (accuracy = 0.982)\n",
    "- Support Vector Machine with RBF Kernel (accuracy = 0.907)\n",
    "\n",
    "\n",
    "The methods that rank in the last two places in terms of accurately classifying train data are:\n",
    "- Logistic Regression (accuracy = 0.795)\n",
    "- Support Vector Machine with Linear Kernel (accuracy = 0.776)\n",
    "\n",
    "The Random Forest classified the training dataset with a very high level of accuracy, because it was able to reduce the amount of overfitting, by averaging across multiple features across the dataset. The Support Vector Machine with RBF Kernel also performed very well, as the size of Gamma allowed most of the data shape to be captured, and prevent overfitting.\n",
    "\n",
    "Logistic Regression and SVM (Linear) did not classify the training dataset with good accuracy. The methods were not robust enough in this scenario to capture the shape of the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Q10. \n",
    "- Exaplain which methods rank in the first two places according to their ability to accurately classify test data, and which two methods perform worst on test dataset? (3 marks)\n",
    "- How do these accuracies compare with the ones reported in Q9? Is this expected, and why (or why not)? (7 marks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------- ANSWER -----------------\n",
    "\n",
    "\n",
    "The methods that rank in the first two places in terms of accurately classifying test data are:\n",
    "- Decision Tree (accuracy = 0.814)\n",
    "- Logistic Regression (accuracy = 0.802)\n",
    "\n",
    "The methods that rank in the last two places in terms of accurately classifying test data are:\n",
    "- Random Forest (accuracy = 0.768)\n",
    "- Support Vector Machine with RBF Kernel (accuracy = 0.750)\n",
    "\n",
    "In terms of comparison, it is interesting to note that the top 2 ranking methods for classifying training data, are also the bottom 2 ranking methods for classifying test data. Though there is always going to be some variation between training and test data accuracy, such a drastic change in rankings was not expected, given how well the methods performed on the training dataset. This highlights the importance of ensuring methods are always tested on the test dataset before deployment. \n",
    "\n",
    "Logistic Regression, which was one of the worst performers on the training dataset, was in the top 2 performers of the test dataset, with minimal variation between final accuracy scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
