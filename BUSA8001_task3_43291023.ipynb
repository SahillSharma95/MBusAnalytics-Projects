{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUSA8001 - Programming Task 3  \n",
    "\n",
    "**Assignment Points**: 100  \n",
    "**Assignment Weight**: 10%  \n",
    "**Submission**: Submit your file using the URL on iLearn\n",
    "\n",
    "\n",
    "- Do NOT use `print()` unless explicitly asked to do so\n",
    "    - Marks will be deducted for not following instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 1**  - Total Marks: 7.5\n",
    "\n",
    "**Q1**. Read the credit card dataset from Programming Task 1 into a DataFrame named `df` and  \n",
    "- Rename the columns 'PAY_0' and 'default payment next month' as in Programming Task 2 \n",
    "- Delete 'ID' column    \n",
    "- Print columns of `df`  \n",
    "- Print shape of `df`    \n",
    "(2.5 marks) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AGE', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'EDUCATION', 'LIMIT_BAL', 'MARRIAGE', 'PAY_1', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6', 'SEX', 'payment_default']\n",
      "(30000, 24)\n"
     ]
    }
   ],
   "source": [
    "# ---- provide answer here -----\n",
    "import pandas as pd  # import pandas library \n",
    "column_names = ['ID','LIMIT_BAL','SEX','EDUCATION','MARRIAGE','AGE','PAY_0','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6','BILL_AMT1','BILL_AMT2','BILL_AMT3','BILL_AMT4','BILL_AMT5','BILL_AMT6','PAY_AMT1','PAY_AMT2','PAY_AMT3','PAY_AMT4','PAY_AMT5','PAY_AMT6','default payment next month']\n",
    "\n",
    "df = pd.read_excel('https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls', skiprows=1, names=column_names)\n",
    "\n",
    "df = df.drop(['ID'], axis=1) # delete ID column\n",
    "\n",
    "df = df.rename(columns={\"PAY_0\":\"PAY_1\", \"default payment next month\":\"payment_default\"})\n",
    "\n",
    "print(sorted(df)) # print all columns of df\n",
    "\n",
    "print(df.shape) # prints the shape of df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2**. Feature Engineering - Create additional features and add them to `df` by squaring the following variables  \n",
    "- LIMIT_BAL  \n",
    "- All BILL_AMT variables  \n",
    "- All PAY_AMT variables  \n",
    "\n",
    "Name the new variables by appending `_2` to the existing variables that you transformed, e.g. LIMIT_BAL_2    \n",
    "(5 marks)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- provide answer here -----\n",
    "import numpy as np\n",
    "\n",
    "# LIMIT_BAL squared\n",
    "df['LIMIT_BAL_2'] = np.power((df['LIMIT_BAL']),2)\n",
    "\n",
    "# All BILL_AMT variables squared\n",
    "df['BILL_AMT1_2'] = np.power((df['BILL_AMT1']),2)\n",
    "df['BILL_AMT2_2'] = np.power((df['BILL_AMT2']),2)\n",
    "df['BILL_AMT3_2'] = np.power((df['BILL_AMT3']),2)\n",
    "df['BILL_AMT4_2'] = np.power((df['BILL_AMT4']),2)\n",
    "df['BILL_AMT5_2'] = np.power((df['BILL_AMT5']),2)\n",
    "df['BILL_AMT6_2'] = np.power((df['BILL_AMT6']),2)\n",
    "\n",
    "# All PAY_AMT variables squared\n",
    "df['PAY_AMT1_2'] = np.power((df['PAY_AMT1']),2)\n",
    "df['PAY_AMT2_2'] = np.power((df['PAY_AMT2']),2)\n",
    "df['PAY_AMT3_2'] = np.power((df['PAY_AMT3']),2)\n",
    "df['PAY_AMT4_2'] = np.power((df['PAY_AMT4']),2)\n",
    "df['PAY_AMT5_2'] = np.power((df['PAY_AMT5']),2)\n",
    "df['PAY_AMT6_2'] = np.power((df['PAY_AMT6']),2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\"> \n",
    "\n",
    "**Problem 2.** Cleaning data and dealing with categorical features - Total Marks: 22.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1**. \n",
    "\n",
    "- Print `value_counts()` of 'SEX' column and add dummy variables 'SEX_MALE' and 'SEX_FEMALE' to `df` using `get_dummies()`. Make sure that the original `SEX` variable is removed from `df`. (2.5 marks)\n",
    "\n",
    "- *Carefully* explain how the new variables are constructed. (1.5 marks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    18112\n",
      "1    11888\n",
      "Name: SEX, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ---- provide answer here -----\n",
    "import pandas as pd\n",
    "# prints count of unique values in the SEX column\n",
    "print(df['SEX'].value_counts()) \n",
    "\n",
    "df = pd.get_dummies(df, columns = ['SEX'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming the SEX_1 and SEX_2 dummy variables\n",
    "\n",
    "df = df.rename(columns={\"SEX_1\":\"SEX_MALE\", \"SEX_2\":\"SEX_FEMALE\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**** EXPLANATION OF NEW VARIABLES ****\n",
    "\n",
    "The new dummy variables, SEX_1 and SEX_2, are created through the use of the get_dummies() function in Pandas. This function identifies the unique, categorical values contained in SEX (i.e. 1 and 2), and creates 2 additional variables accordingly, which are appended to the dataset. For example, if the original value of SEX for row 1 was 1, the corresponding value of SEX_2 for row 1 will be 0, however the value for SEX_1 will be 1. The dummy variables will only have values of 0 or 1, and the get_dummies function will remove the original categorical variable 'SEX'. The .rename function was used to then rename SEX_1 and SEX_2 to SEX_MALE and SEX_FEMALE respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2**. Print `value_counts()` of 'MARRIAGE' column, provide its definition, and *carefully* comment on what you notice in relation to the definition of this variable. (2.5 marks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    15964\n",
      "1    13659\n",
      "3      323\n",
      "0       54\n",
      "Name: MARRIAGE, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ---- provide answer here -----\n",
    "print(df['MARRIAGE'].value_counts()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**** WRITTEN ANSWER ****\n",
    "\n",
    "The value_counts() function converts the unique values within a column to categorical, then provides a count of how many of each unique value appears in the column. The unique values in the MARRIAGE column show that the number of rows with MARRIAGE values 0 or 3 are significantly lower than those which contain values of 1 and 2. Given the large discrepancy, for the purposes of this exercise, 0 and 3 will be grouped together as one variable (i.e. MARRIAGE_OTHER). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3**. \n",
    "\n",
    "- Use `get_dummies()` on 'MARRIAGE' and add dummy variables 'MARRIAGE_MARRIED', 'MARRIAGE_SINGLE', 'MARRIAGE_OTHER' to `df`. Allocate all values of 'MARRIAGE' across the 3 newly created features appropriately. Make sure that the orignial 'MARRIAGE' variable is removed from `df`. (5 marks)\n",
    "\n",
    "- Explain how you created the new features and what decisions you had to make. (3.5 marks)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    15964\n",
      "1    13659\n",
      "3      377\n",
      "Name: MARRIAGE, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ---- provide answer here -----\n",
    "\n",
    "# Convert value 0 into the value 3, so that there are only 3 unique values in MARRIAGE\n",
    "\n",
    "df[\"MARRIAGE\"].replace({0: 3}, inplace=True)\n",
    "\n",
    "print(df['MARRIAGE'].value_counts()) \n",
    "\n",
    "# Creation of dummy variables MARRIAGE_MARRIED, MARRIAGE_SINGLE, MARRIAGE_OTHER\n",
    "df = pd.get_dummies(df, columns = ['MARRIAGE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4**. In the column 'EDUCATION', convert values {0, 4, 5, 6} into the value 4. (7.5 marks)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- provide answer here -----\n",
    "\n",
    "# Convert values 0, 4, 5, 6, into the value 4 for EDUCATION column\n",
    "\n",
    "df['EDUCATION'].replace({0: 4, 4:4, 5:4, 6:4}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\"> \n",
    "\n",
    "**Problem 3** Preparing X and y arrays - Total Marks: 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1**. Create `y` from 12,500 consecutive observations starting from observation 1,000, i.e. observation 1,000 is the starting point, of 'payment_default' column from df. Similarly, create `X`  using 12,500 corresponding observatations of all the remaining features in `df` (2.5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- provide answer here -----\n",
    "import numpy as np\n",
    "\n",
    "# 12,500 observations of 'payment_default', starting from obs 1,000 (i.e. 999) into 1D numpy array (y)\n",
    "y = df.loc[999:13498, ['payment_default']].values\n",
    "y = np.ravel(y)\n",
    "\n",
    "# First 12,500 observations of all other features into 2D numpy array X\n",
    "X = df.loc[999:13498, ['LIMIT_BAL',\n",
    "'EDUCATION',\n",
    "'AGE',\n",
    "'PAY_1',\n",
    "'PAY_2',\n",
    "'PAY_3',\n",
    "'PAY_4',\n",
    "'PAY_5',\n",
    "'PAY_6',\n",
    "'BILL_AMT1',\n",
    "'BILL_AMT2',\n",
    "'BILL_AMT3',\n",
    "'BILL_AMT4',\n",
    "'BILL_AMT5',\n",
    "'BILL_AMT6',\n",
    "'PAY_AMT1',\n",
    "'PAY_AMT2',\n",
    "'PAY_AMT3',\n",
    "'PAY_AMT4',\n",
    "'PAY_AMT5',\n",
    "'PAY_AMT6',\n",
    "'LIMIT_BAL_2',\n",
    "'BILL_AMT1_2',\n",
    "'BILL_AMT2_2',\n",
    "'BILL_AMT3_2',\n",
    "'BILL_AMT4_2',\n",
    "'BILL_AMT5_2',\n",
    "'BILL_AMT6_2',\n",
    "'PAY_AMT1_2',\n",
    "'PAY_AMT2_2',\n",
    "'PAY_AMT3_2',\n",
    "'PAY_AMT4_2',\n",
    "'PAY_AMT5_2',\n",
    "'PAY_AMT6_2',\n",
    "'SEX_MALE',\n",
    "'SEX_FEMALE',\n",
    "'MARRIAGE_1',\n",
    "'MARRIAGE_2',\n",
    "'MARRIAGE_3']].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2**. Use an appropriate scikit-learn library we learned in class to create `y_train`, `y_test`, `X_train` and `X_test` by splitting the data into 70% train and 30% test datasets.  \n",
    "Set random_state to 2 and stratify subsamples so that train and test datasets have roughly equal proportions of the target's class labels. (2.5 marks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- provide answer here -----\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 2, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\"> \n",
    "\n",
    "**Problem 4**. Optimize hyperparameters using grid search and SVC - Total Marks: 40\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1**. Use `make_pipeline` to create a pipeline called `pipe_svc` consisting of:   \n",
    "- StandardScaler    \n",
    "- PCA (set random_state to 1)  \n",
    "- SVC (set random_state to 1)  \n",
    "   \n",
    "(10 marks)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('sc', StandardScaler(), 'pca', PCA(random_state=1)),\n",
       "                ('svc', SVC(random_state=1))])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---- provide answer here -----\n",
    "# y variable is already encoded, so no need for encoding. \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "#pipe_svc = make_pipeline(StandardScaler(),\n",
    "                       # PCA(random_state=1),\n",
    "                        #SVC(random_state=1))\n",
    "\n",
    "pipe_svc = make_pipeline(StandardScaler(), PCA(random_state=1), SVC(random_state=1))\n",
    "Pipeline(steps=[('sc', StandardScaler(), 'pca', PCA(random_state=1)), ('svc', SVC(random_state=1))])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2**. Use `GridSearchCV` to create `gs` object, fit the model and tune the following hyperparameters     \n",
    "- SVC $C$ parameter - grid search over the following values [0.1, 1, 10]    \n",
    "- SVC kernel - grid search over 3 alternatives: linear, sigmoid, and rbf  \n",
    "- Number of PCA components - grid search over the following 3 values [1, 4, 9]        \n",
    "- When implementing `GridSearchCV` set the following options (leaving everying else to their default values)    \n",
    "    - accuracy for scoring   \n",
    "    - `refit` to True   \n",
    "    - number of cross-validation folds to 10   \n",
    "    - `n_jobs=-1`     \n",
    "     \n",
    "(20 marks)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8029714285714287\n",
      "{'svc__C': 1.0, 'svc__kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "# ---- provide answer here -----\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_range_SVC = [0.1, 1.0, 10.0]  # range of values for C and gamma\n",
    "param_range_PCA = [1, 4, 9]\n",
    "\n",
    "param_grid2 = [{'svc__C': param_range_SVC,   # range of values for all parameters\n",
    "               'svc__kernel': ['linear']},\n",
    "              {'svc__C': param_range_SVC, \n",
    "               'svc__kernel': ['sigmoid']}, \n",
    "               {'svc__C': param_range_SVC,\n",
    "                'svc__kernel': ['rbf']},\n",
    "              {'pca__n_components': param_range_PCA}]\n",
    "\n",
    "gs = GridSearchCV(estimator=pipe_svc,      # initialise gs object\n",
    "                  param_grid=param_grid2, \n",
    "                  scoring='accuracy', \n",
    "                  refit=True,              # this will refit the best estimator to the whole dataset automatically\n",
    "                  cv=10,\n",
    "                  n_jobs=-1)\n",
    "\n",
    "\n",
    "gs = gs.fit(X_train, y_train)            # fit gs\n",
    "\n",
    "print(gs.best_score_)\n",
    "\n",
    "print(gs.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3**. Using the best model optimised by grid-search print the following   \n",
    "- cross-validation `best_score_`  \n",
    "- accuracy for the training set  \n",
    "- accuracy for the test set    \n",
    "\n",
    "(10 marks)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation best score: 0.8029714285714287\n",
      "Test accuracy: 0.809\n",
      "Train accuracy: 0.813\n"
     ]
    }
   ],
   "source": [
    "# ---- provide answer here -----          # fit gs\n",
    "\n",
    "print(f'Cross-validation best score: {gs.best_score_}')\n",
    "             \n",
    "\n",
    "best_classifier = gs.best_estimator_ \n",
    "print(f'Test accuracy: {best_classifier.score(X_test, y_test):.3f}')\n",
    "\n",
    "print(f'Train accuracy: {best_classifier.score(X_train, y_train):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\"> \n",
    "\n",
    "**Problem 5.** Confusion Matrix - Total marks: 25   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1**. Use the best fitted model of `gs` to print the confusion matrix. (5 marks)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2801  102]\n",
      " [ 614  233]]\n"
     ]
    }
   ],
   "source": [
    "# ---- provide answer here -----\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "pipe_svc.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipe_svc.predict(X_test)\n",
    "\n",
    "confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "print(confmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2**. Plot the confusion matrix, and on its basis compute the True Positive Rate, False Positive Rate and Precision. (10 marks)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 180x180 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive Rate: 0.9648639338615226\n",
      "False Positive Rate: 0.7249114521841794\n",
      "Precision: 0.8202049780380674\n"
     ]
    }
   ],
   "source": [
    "# ---- provide answer here -----\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "\n",
    "# ---------- Plotting \n",
    "fig, ax = plt.subplots(figsize=(2.5, 2.5))\n",
    "ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(confmat.shape[0]):\n",
    "    for j in range(confmat.shape[1]):\n",
    "        ax.text(x=j, y=i, s=confmat[i, j], va='center', ha='center')\n",
    "        \n",
    "\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig('images/06_09.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# TRUE POSITIVE RATE\n",
    "TPR = 2801/(2801+102)\n",
    "\n",
    "# FALSE POSITIVE RATE\n",
    "FPR = 614/(614+233)\n",
    "\n",
    "# PRECISION\n",
    "precision = 2801/(2801+614)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3**. Looking at the confusion matrix values and the three quantities that you computed what is the greatest source of risk to the credit card company should it rely on the predictions constructed by our model optimised for `accuracy`?   \n",
    "Explain your answer in detail. (10 marks)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Provide answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***ANSWER***\n",
    "\n",
    "The greatest source of risk to the Credit Card company, according to the above computed quantities, is the risk of identifying defaults incorrectly, i.e. False Positives. This is otherwise known as a Type 1 statistical error. This model has a very high False Positive rate, of 72.5%. In the above Confusion Matrix, of the 847 individuals that were actually negative, the model incorrectly predicted 614 individuals as actually being positive. This could result in considerable compliance and/or legal implications for the Credit Card company.  \n",
    "\n",
    "In saying that, however, it is also important to consider the fact that the model is also highly effective at identifying True Positives, with a rate of 96.5%. A Precision score of 82.02% also suggests the model is quite strong at identifying True Positives. So, if the model can be further optimised to improve the False Positive Rate, it can indeed prove to be highly effective for the Credit Card company. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
